# Вопросы по аналитике в проекте BioDash

## 1. Методы обнаружения аномалий и выбросов

### 1.1. Isolation Forest
- **Как конкретно вы использовали Isolation Forest?**
  - Какой параметр contamination использовался?
  - Сколько деревьев (n_estimators) было в модели?
  - Как интерпретировались результаты (score_samples)?
  - Почему выбрали именно этот метод для обнаружения аномалий?

### 1.2. DBSCAN
- **Как вы использовали DBSCAN для обнаружения аномалий?**
  - Как определялся параметр eps (автоматически через k-расстояния или вручную)?
  - Какой использовался min_samples?
  - Как интерпретировались результаты кластеризации для обнаружения аномалий?
  - Почему выбрали 75-й перцентиль для определения eps?

### 1.3. Local Outlier Factor (LOF)
- **Как вы использовали LOF?**
  - Какой параметр n_neighbors использовался?
  - Как интерпретировались anomaly_score_lof?
  - В чем преимущества LOF по сравнению с другими методами?

### 1.4. Консенсусный подход
- **Как вы комбинировали результаты разных методов?**
  - Как формировался consensus (anomaly_consensus)?
  - Какие критерии использовались для определения аномалии по консенсусу?
  - Сколько методов должны были согласиться, чтобы значение считалось аномалией?

## 2. Статистический анализ данных

### 2.1. Описательная статистика
- **Какие статистические метрики вы вычисляли?**
  - Среднее, медиана, стандартное отклонение
  - Минимум, максимум, перцентили (25%, 50%, 75%, 95%)
  - Как использовались эти метрики для анализа?

### 2.2. Референсные диапазоны
- **Как вы создавали временные референсные диапазоны?**
  - На основе каких перцентилей (2.5% и 97.5% или другие)?
  - Как обрабатывались выбросы при создании референсов?
  - Почему референсы помечены как PROVISIONAL (временные)?
  - Как планируется замена временных референсов на официальные?

### 2.3. Нормализация и масштабирование
- **Как вы нормализовали данные для машинного обучения?**
  - Использовался ли StandardScaler или MinMaxScaler?
  - Нормализовались ли данные перед применением методов обнаружения аномалий?
  - Как это влияло на результаты?

## 3. Fuzzy Matching и сопоставление названий

### 3.1. RapidFuzz
- **Как вы использовали RapidFuzz для сопоставления названий тестов?**
  - Какие метрики использовались (ratio, partial_ratio, token_sort_ratio, token_set_ratio)?
  - Какой порог схожести (similarity_threshold) использовался по умолчанию?
  - Почему выбрали именно 0.85 как порог?

### 3.2. Нормализация названий
- **Как вы нормализовали названия колонок?**
  - Транслитерация кириллицы в латиницу
  - Приведение к нижнему регистру
  - Удаление цифр и специальных символов
  - Замена пробелов и точек на подчеркивания
  - Как это помогало в сопоставлении?

### 3.3. Кластеризация схожих названий
- **Как вы группировали схожие названия в кластеры?**
  - Какой алгоритм использовался для кластеризации?
  - Как выбиралось имя кластера (самое короткое или первое по алфавиту)?
  - Как это использовалось для маппинга между JSON и Excel?

## 4. Визуализация данных (Frontend)

### 4.1. Chart.js
- **Как вы использовали Chart.js для визуализации?**
  - Какие типы графиков использовались (line, bar, pie, scatter)?
  - Как настраивались опции графиков (responsive, maintainAspectRatio, plugins)?
  - Как обрабатывались tooltips и легенды?

### 4.2. Линейные графики динамики
- **Как вы строили графики динамики параметров?**
  - Как группировались данные по датам?
  - Как отображались референсные значения (норма) на графиках?
  - Как обрабатывались пропущенные значения?
  - Как определялись 3 основных параметра для отображения по умолчанию?

### 4.3. Scatter plots с референсными значениями
- **Как вы строили scatter plots с референсными диапазонами?**
  - Как отображалась зона "нормы" на графике?
  - Как обрабатывались точки с одинаковыми значениями (stacking)?
  - Как определялись цвета для нормальных и аномальных значений?
  - Как добавлялись метки пациентов с обработкой коллизий?

### 4.4. Столбчатые и круговые диаграммы
- **Как вы строили столбчатые диаграммы распределения анализов?**
  - Как определялись топ анализов по заполненности?
  - Как строились графики распределения анализов на пациента?
  - Как форматировались подписи осей и значения?

## 5. Фильтрация и группировка данных

### 5.1. Фильтры на фронтенде
- **Как работают фильтры по пациенту, дате и типу анализа?**
  - Как данные фильтруются на клиенте vs на сервере?
  - Как обрабатываются пустые значения в фильтрах?
  - Как работает фильтр STAT-проб (срочных анализов)?

### 5.2. Группировка анализов
- **Как анализы группируются по категориям?**
  - Биохимия (biochemistry)
  - Общий анализ крови (blood_count)
  - Липидный профиль (lipid_profile)
  - Антропометрия (anthropometry)
  - Инфекции (infections)
  - Воспаление/ревматология (inflammation)
  - Как определяется категория анализа по test_code?

### 5.3. Определение отклонений от нормы
- **Как вы определяли, что значение выходит за пределы нормы?**
  - Сравнение с референсными диапазонами (norm_min, norm_max)
  - Как обрабатывались односторонние диапазоны (только min или только max)?
  - Как определялся статус (HIGH, LOW, NORMAL)?

## 6. Обработка данных и предобработка

### 6.1. Очистка данных
- **Как вы очищали данные от артефактов?**
  - Как определялись подозрительные значения (suspect values)?
  - Какие правила использовались для обнаружения артефактов?
  - Как обрабатывались пропущенные значения (NaN, null, пустые строки)?

### 6.2. Преобразование форматов данных
- **Как вы преобразовывали данные из длинного формата в широкий?**
  - Как определялось, в каком формате данные (длинный или широкий)?
  - Как обрабатывались метаданные (дата, пол, возраст) при преобразовании?
  - Как группировались данные по patient_id?

### 6.3. Единицы измерения
- **Как вы определяли единицы измерения для анализов?**
  - Использовался ли маппинг единиц измерения (getTestUnit)?
  - Как единицы измерения добавлялись к значениям на фронтенде?
  - Откуда брались единицы измерения (из JSON метаданных или определялись автоматически)?

## 7. Производительность и оптимизация

### 7.1. Обработка больших объемов данных
- **Как вы оптимизировали работу с большими таблицами?**
  - Как реализована пагинация (показ только 10 строк по умолчанию)?
  - Как обрабатываются запросы к API для больших датасетов?
  - Используется ли кэширование данных?

### 7.2. Рендеринг графиков
- **Как вы оптимизировали рендеринг графиков?**
  - Как обрабатывается resize графиков при изменении размера окна?
  - Как уничтожаются предыдущие инстансы графиков перед созданием новых?
  - Используется ли lazy loading для графиков?

## 8. Интеграция фронтенда и бэкенда

### 8.1. API взаимодействие
- **Как данные передаются между фронтендом и бэкендом?**
  - Какие эндпоинты используются для получения данных?
  - Как обрабатываются ошибки API?
  - Как определяется API_BASE (локально vs на хостинге)?

### 8.2. Формат данных
- **В каком формате передаются данные?**
  - Структура JSON для данных пациента
  - Формат данных для графиков (charts)
  - Формат данных для групп анализов (groups)
  - Формат данных для аномальных тестов (abnormal_tests)

## 9. Специфические вопросы по реализации

### 9.1. Округление значений
- **Как вы округляли значения для отображения?**
  - Почему используется округление до 2 знаков после запятой (roundToHundredths)?
  - Как форматируются числа в tooltips и на графиках?

### 9.2. Обработка дат
- **Как вы обрабатывали даты?**
  - Какие форматы дат поддерживаются?
  - Как даты форматируются для отображения (DD.MM.YYYY)?
  - Как определяется последняя дата анализа?

### 9.3. Цветовое кодирование
- **Как вы использовали цвета для индикации статуса?**
  - Зеленый для нормальных значений
  - Красный для высоких значений (HIGH)
  - Синий для низких значений (LOW)
  - Желтый для зоны нормы на scatter plots
  - Как это помогает в визуальном анализе?

## 10. Метрики качества и валидация

### 10.1. Валидация данных
- **Как вы валидировали входные данные?**
  - Проверка формата файлов (CSV, JSON, XLSX)
  - Проверка наличия обязательных колонок
  - Проверка типов данных

### 10.2. Оценка качества аналитики
- **Как вы оценивали качество обнаружения аномалий?**
  - Есть ли метрики точности, полноты, F1-score?
  - Как валидировались результаты методов обнаружения аномалий?
  - Есть ли ручная проверка результатов?

## 11. Документация и воспроизводимость

### 11.1. Параметры методов
- **Где документированы все параметры методов?**
  - Пороги схожести для fuzzy matching
  - Параметры моделей машинного обучения
  - Статистические пороги для определения аномалий

### 11.2. Воспроизводимость результатов
- **Как обеспечивается воспроизводимость результатов?**
  - Используются ли фиксированные random_state?
  - Документированы ли все параметры в коде?
  - Есть ли примеры использования методов?

## 12. Будущие улучшения

### 12.1. Планируемые улучшения
- **Какие улучшения планируются в аналитике?**
  - Замена временных референсов на официальные
  - Добавление новых методов обнаружения аномалий
  - Улучшение точности fuzzy matching
  - Оптимизация производительности

### 12.2. Масштабируемость
- **Как система будет масштабироваться?**
  - Обработка больших объемов данных
  - Параллельная обработка
  - Кэширование результатов

